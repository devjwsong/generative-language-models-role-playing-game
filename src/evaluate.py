from kani.engines.openai import OpenAIEngine
from agents.manager import GameManager
from utils import print_system_log, select_options, check_init_types
from constants import ASSISTANT_INSTRUCTION
from utils import log_break
from sentence_transformers import SentenceTransformer
from typing import Dict

import asyncio
import argparse
import json
import logging
import torch

log = logging.getLogger("kani")
message_log = logging.getLogger("kani.messages")


def evaluate_init(manager: GameManager, scene: Dict):
    async def test():
        try:
            await manager.init_scene(scene)
            check_init_types(manager)
        except json.decoder.JSONDecodeError:
            return 0.0
        except KeyError:
            return 0.2
        except AssertionError:
            return 0.5
        else:
            # TODO: How to export the export results?
            print()
            manager.show_scene()
            options = [
                {'score': 1.0, 'description': "Perfect contents."},
                {'score': 0.8, 'description': "Suboptimal contents."}
            ]
            selected = select_options(options)
            return selected['score']
    asyncio.run(test())


def evaluate_rules(manager: GameManager):
    # The list of test questions.
    questions = [
        'What is the difference between a test and an action scene?',
        'List all properties that one player character can have during the game.',
        'What is required for evaluating if the NPC says or behaves properly during the game?',
        'Assume that the difficulty of a test is 5. If two more players are going to help the test with their traits, what is the final difficulty value?',
        'If the inventory of a player is full and there is a item the player wants to have. What should the player do?',
        'What is this action scene initiated by a player different from the one by the Goblin King?',
        "How long does an NPC stay in the player's party after it joins? Give the specific time amount.",
        'Which amount of the overall time limit in the Labyrinth is?',
        'Describe how the game manager can end the current scene.',
        'How does an action is terminated?',
        'What is the condition that the player can pass the test if the difficulty value is 3?',
        'What is the valid range of difficulty number?',
        'How does the Goblin King use the random tables during the game?',
        'What is the maximum number of items that one player can hold?',
        'If other players decide to help the one who is going to do a test, describe how the test changes depending on the traits or flaws.',
        'What is the effect of the items in the Labyrinth?',
        'What happens if the Goblin King does not notify the decrease of remaining time at every minute?',
        'Assume that the difficulty of a test is 4. If three more players are going to help the test with their traits, what is the final difficulty value?',
        'How many actions are allowed per player at each turn?',
        'How much is the time limit for each player turn during an action scene?',
        'What should the Goblin King do if a player tries to speak with an NPC?',
        'If the result from a dice is 1, what is the possible difficulty range of a test the player can win?',
        "How can we make the NPCs to stay in the player's group after the Goblin King appears in the scene?",
        'What is the role of the Goblin King during an action scene?',
        'If a player wants to talk with an NPC whose attributes have not been generated by the Goblin King before, what should the Goblin King do?',
        'What is the difficulty of a test for checking if an NPC leaves the party?'
    ]

    # The list of the user scores.
    options = [
        {'score': 1.0, 'description': "Perfectly correct."},
        {'score': 0.5, 'description': "Partially correct. (e.g. dropping essential information, faking up the false rules...)"},
        {'score': 0.0, 'description': "Completely wrong."}
    ]
    scores = []

    async def test():
        for q, question in enumerate(questions):
            query = f"Answer the following question according to the Labyrinth's rules.\n{question}"
            response = await manager.chat_round_str(query, include_functions=False)
            print()
            print(f"QUESTION {q+1}: {question}")
            print(f"ANSWER: {response}")

            # Recording the user score.
            print("Select the score for the given response.")
            selected = select_options(options)
            scores.append(selected['score'])

            # Clearing the chat history.
            manager.chat_history = []
    asyncio.run(test())

    # TODO: How to export the export results?
    return scores


if __name__=='__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--eval_name', type=str, required=True, help="The name of the evaluation task.")
    parser.add_argument('--model_idx', type=str, required=True, help="The index of the model.")
    parser.add_argument('--rule_injection', type=str, default=None, help="The rule injection policy.")
    parser.add_argument('--scene_idx', type=int, help="The index of the scene for the initialization evaluation.")

    args = parser.parse_args()

    assert args.eval_name in ['init', 'rules', 'gameplay'], "Specify the correct evaluation task name."

    # Setting the default arguments for each evaluation task.
    if args.eval_name == 'init':
        if args.rule_injection is None or args.rule_injection != 'full':
            print_system_log("THE EVALUATION FOR SCENE INITIALZIATION ALWAYS USES THE FULL RULE INJECTION.")
            args.rule_injection = 'full'
    else:
        assert args.rule_injection in [None, 'full', 'retrieval'], "Either specify an available rule injection option: 'full' / 'retrieval', or leave it as None."
    args.concat_policy = 'simple'
    args.max_turns = None
    args.summarization = False
    args.summ_period = None
    args.clear_raw_logs = False

    # Creating the engine.
    api_key = input("Enter the API key for OpenAI API: ")
    log_break()
    engine = OpenAIEngine(api_key, model=args.model_idx)

    # Intializing the sentence encoder if the concatenation policy is retrieval or the rule injection policy is retrieval.
    encoder = None
    if args.rule_injection == 'retrieval':
        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')
        encoder = SentenceTransformer('all-mpnet-base-v2').to(device)

    # Initializing the game manager.
    system_prompt = ' '.join(ASSISTANT_INSTRUCTION)
    manager = GameManager(
        main_args=args,
        encoder=encoder,
        engine=engine, 
        system_prompt=system_prompt
    )

    if args.eval_name == 'init':
        # Loading the scene file.
        with open("data/scenes.json", 'r') as f:
            scenes = json.load(f)

        assert args.scene_idx is not None, "The scene index should be provided."
        assert 0 <= args.scene_idx < len(scenes), "The scene index is not valid."

        evaluate_init(manager, scenes[args.scene_idx])
    elif args.eval_name == 'rules':
        evaluate_rules(manager)
