from kani.engines.openai import OpenAIEngine
from agents.manager import GameManager
from utils import print_system_log, select_options, check_init_types
from constants import ASSISTANT_INSTRUCTION
from utils import log_break
from sentence_transformers import SentenceTransformer
from typing import Dict

import asyncio
import argparse
import json
import logging
import torch

log = logging.getLogger("kani")
message_log = logging.getLogger("kani.messages")


def evaluate_init(manager: GameManager, scene: Dict):
    async def test():
        try:
            await manager.init_scene(scene)
            check_init_types(manager)
        except json.decoder.JSONDecodeError:
            return 0.0
        except KeyError:
            return 0.2
        except AssertionError:
            return 0.5
        else:
            # TODO: How to export the export results?
            print()
            manager.show_scene()
            options = [
                {'score': 1.0, 'description': "Perfect contents."},
                {'score': 0.8, 'description': "Suboptimal contents."}
            ]
            selected = select_options(options)
            return selected['score']
    asyncio.run(test())


def evaluate_rules(manager: GameManager):
    # The list of test questions.
    questions = [
        'What is the difference between a test and an action scene?',
        'List all properties that one player character can have during the game.',
        'What is required for evaluating if the NPC says or behaves properly during the game?',
        'Assume that the difficulty of a test is 5. If two more players are going to help the test with their traits, what is the final difficulty value?',
        'If the inventory of a player is full and there is a item the player wants to have. What should the player do?',
        'What is this action scene initiated by a player different from the one by the Goblin King?',
        "How long does an NPC stay in the player's party after it joins? Give the specific time amount.",
        'Which amount of the overall time limit in the Labyrinth is?',
        'Describe how the game manager can end the current scene.',
        'How does an action is terminated?',
        'What is the condition that the player can pass the test if the difficulty value is 3?',
        'What is the valid range of difficulty number?',
        'How does the Goblin King use the random tables during the game?',
        'What is the maximum number of items that one player can hold?',
        'If other players decide to help the one who is going to do a test, describe how the test changes depending on the traits or flaws.',
        'What is the effect of the items in the Labyrinth?',
        'What happens if the Goblin King does not notify the decrease of remaining time at every minute?',
        'Assume that the difficulty of a test is 4. If three more players are going to help the test with their traits, what is the final difficulty value?',
        'How many actions are allowed per player at each turn?',
        'How much is the time limit for each player turn during an action scene?',
        'What should the Goblin King do if a player tries to speak with an NPC?',
        'If the result from a dice is 1, what is the possible difficulty range of a test the player can win?',
        "How can we make the NPCs to stay in the player's group after the Goblin King appears in the scene?",
        'What is the role of the Goblin King during an action scene?',
        'If a player wants to talk with an NPC whose attributes have not been generated by the Goblin King before, what should the Goblin King do?',
        'What is the difficulty of a test for checking if an NPC leaves the party?'
    ]

    # The list of the user scores.
    options = [
        {'score': 1.0, 'description': "Perfectly correct."},
        {'score': 0.5, 'description': "Partially correct. (e.g. dropping essential information, faking up the false rules...)"},
        {'score': 0.0, 'description': "Completely wrong."}
    ]
    scores = []

    async def test():
        for q, question in enumerate(questions):
            query = f"Answer the following question according to the Labyrinth's rules.\n{question}"
            response = await manager.chat_round_str(query, include_functions=False)
            print()
            print(f"QUESTION {q+1}: {question}")
            print(f"ANSWER: {response}")

            # Recording the user score.
            print("Select the score for the given response.")
            selected = select_options(options)
            scores.append(selected['score'])

            # Clearing the chat history.
            manager.chat_history = []
    asyncio.run(test())

    # TODO: How to export the export results?
    return scores


if __name__=='__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--eval_task', type=str, required=True, help="The name of the evaluation task.")
    parser.add_argument('--automated_evaluator', action='store_true', help="Setting whether to use another language model for evaluation.")
    parser.add_argument('--eval_model_idx', type=str, help="The name of the model which is used for the automatic evaluation.")

    # Arguments for the gameplay evalaution.
    parser.add_argument('--gameplay_path', type=str, help="The path of the file which has the whole game play data.")

    # Arguments for the scene intialization evaluation.
    parser.add_argument('--scene_path', type=str, help="The path of the file which has the initialized scene information.")

    # Arguments for the 
    parser.add_argument('--target_model_idx', type=str, help="The index of the model which should be evaluated.")
    parser.add_argument('--rule_injection', type=str, default=None, help="The rule injection policy.")

    args = parser.parse_args()

    assert args.eval_task in ['gameplay', 'scene_init', 'rules'], "Specify the correct evaluation task name."
    if args.automated_evaluator:
        assert args.eval_model_idx is not None, "You should specify the model which will be an AI evaluator."

    # Setting the engine for automated evaluation or evaluation of rule understanding.
    if args.automated_evalutor or args.eval_task == 'rules':
        api_key = input("Enter the API key for OpenAI API: ")
        log_break()
        engine = OpenAIEngine(api_key, model=args.target_model_idx)

    # Setting & Validting the arguments for each evaluation task.
    if args.eval_task == 'gameplay':
        assert args.gameplay_path is not None, "You should specify the gameplay data you want to evaluate."
    if args.eval_task == 'scene_init':
        assert args.scene_path is not None, "You should specify the initialized scene data you want to evaluate."
    if args.eval_task == 'rules':
        assert args.target_model_idx is not None, "You should specify the model you want to test."
        assert args.rule_injection in [None, 'full', 'retrieval'], "Either specify an available rule injection option: 'full' / 'retrieval', or leave it as None."

        # Setting the sentence encoder for the rule embedding.
        encoder = None
        if args.rule_injection == 'retrieval':
            device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')
            encoder = SentenceTransformer('all-mpnet-base-v2').to(device)

        args.concat_policy = 'simple'
        args.max_num_msgs = None
        args.summarization = False
        args.summ_period = None
        args.clear_raw_logs = False
        args.automated_player = False

        # Initializing the target game manager.
        system_prompt = ' '.join(ASSISTANT_INSTRUCTION)
        target_manager = GameManager(
            main_args=args,
            encoder=encoder,
            engine=engine, 
            system_prompt=system_prompt
        )
